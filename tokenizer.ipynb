{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4868804-0521-4c40-9a32-36ea1bda3d80",
   "metadata": {},
   "source": [
    "## 1. The Vocabulary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "6c01d051-3cfd-466d-a3e5-5b1fc51651c1",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pdfminer.high_level import extract_text\n",
    "from pdfminer.pdfparser import PDFParser\n",
    "from pdfminer.pdfdocument import PDFDocument\n",
    "from pdfminer.pdfpage import PDFPage\n",
    "import re \n",
    "import os \n",
    "import shutil\n",
    "import torch"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e61c6d55-759a-43e2-b929-94c53bedf44a",
   "metadata": {},
   "source": [
    "#### collecting the vocabulary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa075b26-cf03-48bd-8351-eee53828053a",
   "metadata": {},
   "source": [
    "##### This can be any data, can be from internet or you own data. I have used a book named pride and prejudice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "476dd895-9d55-4cc6-b3ec-4e046dbea731",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File Paths\n",
    "doc_name = \"pride_and_prejudice\"\n",
    "\n",
    "path = \"pride_and_prejudice.pdf\"\n",
    "text_filename = \"pride_and_prejudice.txt\"\n",
    "filepath_clean = f\"pride_and_prejudice_clean.txt\"\n",
    "filepath_words = f\"pride_and_prejudice_words.txt\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1abe860-ef46-4198-b618-a0f6f7e22b77",
   "metadata": {},
   "source": [
    "## 2. Data Cleaning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "228bc8af-5fea-47a3-921e-a30d31dbafe8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split text by sentence\n",
    "\n",
    "import os\n",
    "from PyPDF2 import PdfReader\n",
    "from pdfminer.high_level import extract_text\n",
    "\n",
    "document = PdfReader(open(path, \"rb\"))  # replace with your PDF file\n",
    "\n",
    "\n",
    "all_pages_text = []\n",
    "page_num = 0\n",
    "\n",
    "for i in range(len(document.pages)):\n",
    "    # Convert page to PDF File Writer object\n",
    "    page = document.pages[i]\n",
    "    \n",
    "    # Extract text from page\n",
    "    page_text = page.extract_text()\n",
    "\n",
    "    start_page = i - 1\n",
    "\n",
    "    all_pages_text.append(page_text)\n",
    "\n",
    "with open(text_filename, 'w', encoding=\"utf-8\") as file:\n",
    "    for item in all_pages_text:\n",
    "        file.write(str(item) + '\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "a79c6439-7aa6-416d-89c1-169e470749de",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/mansoor/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    }
   ],
   "source": [
    "# PRE-PROCESS THE TEXT\n",
    "import os\n",
    "import pandas as pd\n",
    "import nltk\n",
    "\n",
    "# Download the punkt tokenizer models\n",
    "nltk.download('punkt')\n",
    "\n",
    "def file_to_sentences(filepath):\n",
    "    \"\"\"\n",
    "    Given a filepath, read the text file and split it into sentences.\n",
    "    Returns a list of sentences.\n",
    "    \"\"\"\n",
    "    with open(filepath, 'r', encoding='utf-8') as file:\n",
    "        content = file.read()\n",
    "        sentences = nltk.tokenize.sent_tokenize(content)\n",
    "    return sentences\n",
    "\n",
    "def add_sentences_to_dataframe(sentences, dataframe):\n",
    "    \"\"\"\n",
    "    Add sentences to the given dataframe.\n",
    "    \"\"\"\n",
    "    for sentence in sentences:\n",
    "        dataframe = dataframe.append({'Sentence': sentence}, ignore_index=True)\n",
    "    return dataframe\n",
    "\n",
    "# Initialize an empty DataFrame with one column \"Sentence\"\n",
    "df = pd.DataFrame(columns=[\"text\"])\n",
    "\n",
    "# Initialize a list to hold all sentences\n",
    "all_sentences = []\n",
    "\n",
    "# Set the directory where the text files are located\n",
    "filepath = f\"{doc_name}.txt\"\n",
    "all_sentences.extend(file_to_sentences(filepath))\n",
    "\n",
    "\n",
    "# Convert the list of sentences to a DataFrame\n",
    "df = pd.DataFrame(all_sentences, columns=[\"Sentence\"])\n",
    "\n",
    "# Save the DataFrame to a CSV file\n",
    "df.to_csv(f\"{doc_name}_sentences.csv\", index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "12b08b3b-0d18-4a21-9ec6-97dcc11ee149",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The text has been cleaned and saved to pride_and_prejudice_clean.txt.\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import unicodedata\n",
    "\n",
    "# Function to convert to ASCII\n",
    "def to_ascii(text):\n",
    "    normalized = unicodedata.normalize('NFKD', text)\n",
    "    return normalized.encode('ascii', 'ignore').decode('ascii')\n",
    "\n",
    "# Function to clean the text\n",
    "def clean_text(text):\n",
    "    # Convert to lowercase and to ASCII\n",
    "    text = to_ascii(text.lower())\n",
    "    # Keep only alphabetic characters and spaces\n",
    "    text = re.sub(r'[^a-z\\s]+', ' ', text)\n",
    "    # Normalize spaces to a single space\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "# Read the content of the file\n",
    "\n",
    "\n",
    "with open(filepath, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Clean the content\n",
    "cleaned_content = clean_text(content)\n",
    "\n",
    "# Save the cleaned content back to a file\n",
    "\n",
    "with open(filepath_clean, 'w', encoding='ascii') as file:\n",
    "    file.write(cleaned_content)\n",
    "\n",
    "print(f\"The text has been cleaned and saved to {filepath_clean}.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "e642abff-4d7f-4d57-bceb-e637abb403ad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total unique words: 7526\n",
      "The list of unique words has been saved to pride_and_prejudice_words.txt\n"
     ]
    }
   ],
   "source": [
    "# TOKENIZE THE TEXT\n",
    "\n",
    "# Function to extract unique words from the text\n",
    "def extract_unique_words(text):\n",
    "    # Split text into words based on whitespace\n",
    "    words = text.split()\n",
    "    # Use a set to remove duplicates, extracting the unique words\n",
    "    unique_words = set(words)\n",
    "    return unique_words\n",
    "\n",
    "# Read the cleaned content of the file\n",
    "with open(filepath_clean, 'r', encoding='utf-8') as file:\n",
    "    content = file.read()\n",
    "\n",
    "# Get the list of unique words\n",
    "unique_words_set = extract_unique_words(content)\n",
    "unique_words_list = sorted(list(unique_words_set))  # Convert to a sorted list if order is needed\n",
    "\n",
    "# Optionally save the unique words to a file\n",
    "with open(filepath_words, 'w', encoding='utf-8') as file:\n",
    "    for word in unique_words_list:\n",
    "        file.write(word + '\\n')\n",
    "\n",
    "print(f\"Total unique words: {len(unique_words_list)}\")\n",
    "print(f\"The list of unique words has been saved to {filepath_words}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "77b1f2c4-bc0b-485b-ab10-bca036ecd188",
   "metadata": {},
   "outputs": [],
   "source": [
    "lines_list = []\n",
    "\n",
    "# Open the file for reading ('r')\n",
    "with open(filepath_words, 'r', encoding='utf-8') as file:\n",
    "    # Read all lines in the file and add them to the list\n",
    "    lines_list = file.readlines()\n",
    "\n",
    "# Now lines_list contains all the lines from the file\n",
    "# If you want to remove newline characters from the end of each line, you can do:\n",
    "unique_words = [line.strip() for line in lines_list]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e44284c2-2d3d-4e33-8e2b-2005c0104aac",
   "metadata": {},
   "source": [
    "### Special Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dcadf4d8-eb90-425b-a358-f2d94bc66016",
   "metadata": {},
   "outputs": [],
   "source": [
    "list_as_text = ' '.join(unique_words)\n",
    "start_token = '[S]'\n",
    "end_token = '[EOS]'\n",
    "unknown_token = '[UNK]'\n",
    "pad_token = '[PAD]'  # Add the PAD token"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "42611654-de78-409d-9542-900c6d010652",
   "metadata": {},
   "outputs": [],
   "source": [
    "# A tokenizer function to remove special characters for simple tokenizer, \n",
    "#in actual production we need to consider them\n",
    "\n",
    "def tokenize(text):\n",
    "    # Use regular expression to separate words from periods and commas\n",
    "    return re.findall(r\"[\\w']+|[.,!?]\", text.lower())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "5e0eba20-b732-4b88-aa67-47137b8fec37",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['this', 'is', 'a', 'text', 'of', 'our', 'tokenizer']\n"
     ]
    }
   ],
   "source": [
    "text = \"This is a text of our tokenizer\" \n",
    "\n",
    "tokenized_text = tokenize(text) \n",
    "\n",
    "print(tokenized_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "ecdcf0e0-575b-4418-b98e-bbaa54a4a4b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "tokens = tokenize(list_as_text)\n",
    "unique_tokens = list(set(tokens))\n",
    "unique_tokens.sort()\n",
    "# Ensure special tokens are at the beginning\n",
    "unique_tokens = [token for token in unique_tokens if token not in (start_token, end_token, unknown_token, pad_token)] + \\\n",
    "                [start_token, end_token, unknown_token, pad_token]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "id": "be98b41b-a102-44e3-84d0-3c444cdbfffa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# helper functions to go from token to id, and from id to token\n",
    "word2idx = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "\n",
    "idx2word = {idx: token for token, idx in word2idx.items()}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2d41efed-8b7a-4f59-a69b-8c100e9ddcd5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First 10 tokens: ['a', 'ab', 'abatement', 'abeth', 'abhor', 'abhorrence', 'abhorrent', 'abide', 'abiding', 'abilities']\n",
      "Last 10 tokens: ['youunable', 'youwant', 'youwas', 'youwere', 'youwish', 'zle', '[S]', '[EOS]', '[UNK]', '[PAD]']\n"
     ]
    }
   ],
   "source": [
    "# Print the first 10 elements\n",
    "print(\"First 10 tokens:\", unique_tokens[:10])\n",
    "\n",
    "# Print the last 10 elements\n",
    "print(\"Last 10 tokens:\", unique_tokens[-10:])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "7040e4af-7928-4b1e-9e95-facc59ff9b8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "start_token_id = word2idx[start_token]  # Save the index of the PAD token\n",
    "end_token_id = word2idx[end_token]  # Save the index of the PAD token\n",
    "unknown_token_id = word2idx[unknown_token]  # Save the index of the PAD token\n",
    "pad_token_id = word2idx[pad_token]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "58139f68-3829-4aa3-869d-22d46f7ed79e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start_token:   [S]   - Id: 7526\n",
      "end_token:     [EOS] - Id: 7527\n",
      "unknown_token: [UNK] - Id: 7528\n",
      "pad_token:     [PAD] - Id: 7529\n"
     ]
    }
   ],
   "source": [
    "print(f\"start_token:   {start_token}   - Id: {start_token_id}\")\n",
    "print(f\"end_token:     {end_token} - Id: {end_token_id}\")\n",
    "print(f\"unknown_token: {unknown_token} - Id: {unknown_token_id}\")\n",
    "print(f\"pad_token:     {pad_token} - Id: {pad_token_id}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f790a6db-3aab-417e-9347-19a44b8dc40a",
   "metadata": {},
   "source": [
    "### ENCODE function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "3a1b2752-6928-451f-aaf3-495e7c13617d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def encode(text, max_length=None, truncation=False, return_tensors=False):\n",
    "    \"\"\"\n",
    "    Encode the text into a sequence of token IDs, with optional truncation.\n",
    "\n",
    "    Parameters:\n",
    "    - text: The text to encode.\n",
    "    - max_length: The maximum length of the token sequence after encoding.\n",
    "    - truncation: Whether to truncate the sequence to max_length.\n",
    "    \n",
    "    Returns:\n",
    "    - A list of token IDs representing the encoded text.\n",
    "    \"\"\"\n",
    "    tokens = tokenize(text)\n",
    "    encoded_tokens = [word2idx.get(token, word2idx[unknown_token]) for token in tokens]\n",
    "\n",
    "    # Prepend the start token ID and append the end token ID\n",
    "    encoded_tokens = [word2idx[start_token]] + encoded_tokens + [word2idx[end_token]]\n",
    "\n",
    "    # Handle truncation\n",
    "    if truncation and max_length is not None:\n",
    "        # Truncate the sequence if it's longer than max_length\n",
    "        encoded_tokens = encoded_tokens[:max_length - 1] + [word2idx[end_token]]\n",
    "\n",
    "    # Convert to tensor if return_tensors is True\n",
    "    if return_tensors:\n",
    "        encoded_tokens = torch.tensor([encoded_tokens])  # Adding a batch dimension\n",
    "\n",
    "    return encoded_tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "537de16e-087a-4d5a-ae71-a2b475883f15",
   "metadata": {},
   "source": [
    "## 3. Tokenizer"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c866c920-8bdf-4559-bfc0-77e0d8be5a7b",
   "metadata": {},
   "source": [
    "### Decode Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "48fc0123-9d77-4997-90f0-48029e6ee8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def decode(indices, skip_special_tokens=False):\n",
    "    # Ensure indices is a list of integers, not a list of tensors\n",
    "    if isinstance(indices, torch.Tensor):\n",
    "        indices = indices.tolist()\n",
    "    \n",
    "    # Define a set of all special token ids you want to skip\n",
    "    special_token_ids = set()\n",
    "    if skip_special_tokens:\n",
    "        special_token_ids.update([\n",
    "            start_token_id,\n",
    "            end_token_id,\n",
    "            unknown_token_id,\n",
    "            pad_token_id,\n",
    "            # Add any other special token ids you have\n",
    "        ])\n",
    "    \n",
    "    # Use a list comprehension to filter out all special tokens\n",
    "    tokens = [idx2word[idx] for idx in indices if idx not in special_token_ids]\n",
    "    \n",
    "    # Join the tokens into a single string with spaces\n",
    "    return ' '.join(tokens)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "e384ebe6-a8f2-4c42-97f5-4a68e32d0f45",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_ids_to_tokens(token_ids):\n",
    "    \"\"\"\n",
    "    Convert a list of token IDs to their corresponding tokens.\n",
    "    \n",
    "    Parameters:\n",
    "    - token_ids: A list of integers representing token IDs.\n",
    "    \n",
    "    Returns:\n",
    "    - tokens: A list of string tokens corresponding to the input IDs.\n",
    "    \"\"\"\n",
    "    tokens = [idx2word.get(token_id, unknown_token) for token_id in token_ids]\n",
    "    \n",
    "    return tokens"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "44415402-4c90-48b3-a346-47c8d1a84e93",
   "metadata": {},
   "source": [
    "## Tokenizer Function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "id": "08a5ebc2-c802-4cb2-ad6d-04687e382395",
   "metadata": {},
   "outputs": [],
   "source": [
    "class SimpleTokenizer:\n",
    "    def __init__(self, text):\n",
    "        # Define special tokens\n",
    "        self.start_token = '[S]'\n",
    "        self.end_token = '[EOS]'\n",
    "        self.unknown_token = '[UNK]'\n",
    "        self.pad_token = '[PAD]'  # Add the PAD token\n",
    "        \n",
    "        # Tokenize the input text and include special tokens\n",
    "        tokens = self.tokenize(text)\n",
    "        unique_tokens = list(set(tokens))\n",
    "        \n",
    "        # Sort tokens to ensure consistent indexing (except for special tokens)\n",
    "        unique_tokens.sort()\n",
    "        # Ensure special tokens are at the beginning\n",
    "        unique_tokens = [token for token in unique_tokens if token not in (self.start_token, self.end_token, self.unknown_token, self.pad_token)] + \\\n",
    "                        [self.start_token, self.end_token, self.unknown_token, self.pad_token]\n",
    "        \n",
    "        # Assign an index to each unique token\n",
    "        self.word2idx = {token: idx for idx, token in enumerate(unique_tokens)}\n",
    "        self.idx2word = {idx: token for token, idx in self.word2idx.items()}\n",
    "\n",
    "        self.start_token_id = self.word2idx[self.start_token]  # Save the index of the PAD token\n",
    "        self.end_token_id = self.word2idx[self.end_token]  # Save the index of the PAD token\n",
    "        self.unknown_token_id = self.word2idx[self.unknown_token]  # Save the index of the PAD token\n",
    "        self.pad_token_id = self.word2idx[self.pad_token]\n",
    "\n",
    "    def tokenize(self, text):\n",
    "        # Use regular expression to separate words from periods and commas\n",
    "        return re.findall(r\"[\\w']+|[.,!?]\", text.lower())\n",
    "\n",
    "    def encode(self, text, max_length=None, truncation=False, return_tensors=False):\n",
    "        \"\"\"\n",
    "        Encode the text into a sequence of token IDs, with optional truncation.\n",
    "\n",
    "        Parameters:\n",
    "        - text: The text to encode.\n",
    "        - max_length: The maximum length of the token sequence after encoding.\n",
    "        - truncation: Whether to truncate the sequence to max_length.\n",
    "        \n",
    "        Returns:\n",
    "        - A list of token IDs representing the encoded text.\n",
    "        \"\"\"\n",
    "        tokens = self.tokenize(text)\n",
    "        encoded_tokens = [self.word2idx.get(token, self.word2idx[self.unknown_token]) for token in tokens]\n",
    "\n",
    "        # Prepend the start token ID and append the end token ID\n",
    "        encoded_tokens = [self.word2idx[self.start_token]] + encoded_tokens + [self.word2idx[self.end_token]]\n",
    "\n",
    "        # Handle truncation\n",
    "        if truncation and max_length is not None:\n",
    "            # Truncate the sequence if it's longer than max_length\n",
    "            encoded_tokens = encoded_tokens[:max_length - 1] + [self.word2idx[self.end_token]]\n",
    "\n",
    "        # Convert to tensor if return_tensors is True\n",
    "        if return_tensors:\n",
    "            encoded_tokens = torch.tensor([encoded_tokens])  # Adding a batch dimension\n",
    "\n",
    "        return encoded_tokens\n",
    "\n",
    "    def decode(self, indices, skip_special_tokens=False):\n",
    "        # Ensure indices is a list of integers, not a list of tensors\n",
    "        if isinstance(indices, torch.Tensor):\n",
    "            indices = indices.tolist()\n",
    "        \n",
    "        # Define a set of all special token ids you want to skip\n",
    "        special_token_ids = set()\n",
    "        if skip_special_tokens:\n",
    "            special_token_ids.update([\n",
    "                self.start_token_id,\n",
    "                self.end_token_id,\n",
    "                self.unknown_token_id,\n",
    "                self.pad_token_id,\n",
    "                # Add any other special token ids you have\n",
    "            ])\n",
    "        \n",
    "        # Use a list comprehension to filter out all special tokens\n",
    "        tokens = [self.idx2word[idx] for idx in indices if idx not in special_token_ids]\n",
    "        \n",
    "        # Join the tokens into a single string with spaces\n",
    "        return ' '.join(tokens)\n",
    "        \n",
    "    def convert_ids_to_tokens(self, token_ids):\n",
    "        \"\"\"\n",
    "        Convert a list of token IDs to their corresponding tokens.\n",
    "        \n",
    "        Parameters:\n",
    "        - token_ids: A list of integers representing token IDs.\n",
    "        \n",
    "        Returns:\n",
    "        - tokens: A list of string tokens corresponding to the input IDs.\n",
    "        \"\"\"\n",
    "        tokens = [self.idx2word.get(token_id, self.unknown_token) for token_id in token_ids]\n",
    "        \n",
    "        return tokens\n",
    "    \n",
    "    def convert_tokens_to_ids(self, tokens):\n",
    "        \"\"\"\n",
    "        Convert a list of tokens to their corresponding token IDs.\n",
    "\n",
    "        Parameters:\n",
    "        - tokens: A list of string tokens.\n",
    "\n",
    "        Returns:\n",
    "        - token_ids: A list of integers representing the token IDs.\n",
    "        \"\"\"\n",
    "        token_ids = [self.word2idx.get(token, self.unknown_token_id) for token in tokens]\n",
    "\n",
    "        return token_ids"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "id": "95c64bf9-27d7-47e3-853f-68d022051730",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Encoded: [7526, 7528, 7528, 0, 5742, 3678, 5453, 7528, 7527]\n",
      "Decoded: [S] [UNK] [UNK] a rose is red [UNK] [EOS]\n",
      "PAD token ID: 7529\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Usage example\n",
    "tokenizer = SimpleTokenizer(list_as_text)\n",
    "encoded = tokenizer.encode(\"color ? A rose is red.\")\n",
    "decoded = tokenizer.decode(encoded)\n",
    "\n",
    "print(f\"Encoded: {encoded}\")\n",
    "print(f\"Decoded: {decoded}\")\n",
    "print(f\"PAD token ID: {tokenizer.pad_token_id}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6f385ea6-311b-45c2-a2fb-f91b98250528",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1e8dcd82-52e0-48ed-bc87-140a13075736",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b1957b0-5b7d-42eb-8a87-d5711bae1567",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
